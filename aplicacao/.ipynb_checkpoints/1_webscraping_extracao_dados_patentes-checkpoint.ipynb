{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracao de dados de patentes\n",
    "# url - www.freepatentsonline.com\n",
    "#\n",
    "# Henrique Cursino Vieira\n",
    "# versao 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# webscraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# nuvem de palavras\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cada pagina lista 50 patentes, alterar a variavel n para obter o numero de\n",
    "# patentes de interesse\n",
    "p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome das colunas\n",
    "columns = [\"Match\", \"Document\", \"Document_Title\", \"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for pagina in range(1,p+1):\n",
    "    pagina = str(pagina)\n",
    "    link = \"https://www.freepatentsonline.com/result.html?p={}&sort=relevance&srch=top&query_txt=agronomy&patents_us=on\".format(pagina)\n",
    "    \n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Capturando o conteudo da tag <td> e pulando as duas primeiras tags\n",
    "    listing_table = soup.findAll(\"td\")[2:202]    \n",
    "    \n",
    "    for i in range(0, len(listing_table), 4):\n",
    "\n",
    "    #     print(i)\n",
    "        match, document, document_title, score = listing_table[i:i+4]\n",
    "        rows.append([match, document, document_title, score])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total de linhas:', len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o conteudo\n",
    "listing_table[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de links\n",
    "urls = []\n",
    "for i in range(len(listing_table)):\n",
    "\n",
    "    td = listing_table[i]\n",
    "    a = td.findAll('a')\n",
    "    \n",
    "    if a:\n",
    "        urls.append(\"https://www.freepatentsonline.com\"+a[0].get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraindo os dados dos documentos de patentes\n",
    "tmp = dict()\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    \n",
    "    tmp[i] = []\n",
    "    url_splited = url.split('/')\n",
    "    \n",
    "    if len(url_splited) == 4:\n",
    "        idx = url_splited[3].replace('.html', '')\n",
    "        \n",
    "    if len(url_splited) == 5:\n",
    "        idx = url_splited[4].replace('.html', '')\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    tmp[i].append(['idx', idx])\n",
    "    \n",
    "    for row in soup.findAll(\"div\", {\"class\":\"disp_doc2\"}):\n",
    "        \n",
    "        title_content = row.findAll(\"div\", {\"class\":\"disp_elm_title\"})\n",
    "        text_content = row.findAll(\"div\", {\"class\":\"disp_elm_text\"})\n",
    "        title = None\n",
    "        text = None\n",
    "        \n",
    "        if title_content:\n",
    "            title = title_content[0].text\n",
    "\n",
    "            if text_content:\n",
    "                text = text_content[0].text\n",
    "\n",
    "            tmp[i].append([title, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o conteudo de text\n",
    "tmp[0][2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(tmp)):\n",
    "    \n",
    "    row = [tmp[i][0][1], tmp[i][1][1], tmp[i][2][1]]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(rows, columns = ['idx', 'title_raw', 'text_raw'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text = None):\n",
    "    \n",
    "    processed_text = re.sub('\\n{1,}', '', text).strip().lower()    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processamento dos dados e gerando uma coluna unificando titulo e texto\n",
    "data['title']   = data['title_raw'].apply(lambda x : preprocess_text(x))\n",
    "data['text']    = data['text_raw'].apply(lambda x : preprocess_text(x))\n",
    "data['content'] = data['title'] + \" \" + data['text']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'].tolist()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuvem de palavras\n",
    "text = ', '.join(list(data['content'].values))\n",
    "wordcloud = WordCloud(background_color=\"white\", \n",
    "                      max_words=5000, \n",
    "                      contour_width=3, \n",
    "                      contour_color='steelblue',\n",
    "                      width=1200, \n",
    "                      height=800,\n",
    "                      collocations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.generate(text)\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(filename='wordcloud_preprocess.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('patents_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
